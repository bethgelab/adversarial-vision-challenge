#!/usr/bin/env python3

import argparse
import subprocess
import adversarial_vision_challenge
import numpy as np
import os
import sys
import socket
from tqdm import tqdm
from adversarial_vision_challenge.utils import _wait_for_server_start
from adversarial_vision_challenge.common import check_track, \
            reset_repo2docker_cache

def checkmark():
    print(u' \u2713')


def _get_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        return s.getsockname()[1]


def test_model(directory, no_cache, no_build, gpu):
    check_track(directory, 'nips-2018-avc-robust-model-track')

    image_name = 'avc/model_submission'
    container_name = 'avc_test_model_submission'

    subprocess.check_call('avc-test-setup', shell=True)

    print()
    print('Building docker image of submission')
    print('-----------------------------------')

    print('Submission folder: "{}"'.format(directory))

    # build image
    repo2docker_args = ["--no-run", "--debug"]
    repo2docker_args = ["--no-run", "--debug"]
    if no_build:
        #TODO: Use logger instead of print statements
        print("WARNING: ", "Not building the docker image and assuming it is built and available at : {}".format(image_name))
        repo2docker_args.append("--no-build")
    else:
        if no_cache:
            reset_repo2docker_cache()
            print('Building image (without cache)...')
        else:
            print('Building image from cache (if exists)...', end="")
        repo2docker_args.append(
            "--image-name {}".format(image_name)
        )
        cmd = "crowdai_repo2docker"
        cmd += " "
        cmd += " ".join(repo2docker_args)
        cmd += " "
        cmd += directory
        subprocess.check_call(
            "crowdai-repo2docker --no-run --image-name {} --debug {}".format(
                image_name, directory), shell=True)
        checkmark()

    # remove old container if exists
    containers = str(subprocess.check_output('docker ps -a', shell=True))
    if container_name in containers:
        print('Removing existing submission container...', end="")
        cmd = "docker rm -f {cn}".format(cn=container_name)
        subprocess.check_call(cmd, shell=True)
        checkmark()

    # start model container
    port = _get_free_port()
    cmd = "NV_GPU={gpu} nvidia-docker run --expose={port} -d -p {port}:{port} "
    cmd += "-v {directorypath}:/prd "
    cmd += "-e GPU={gpu} -e MODEL_PORT={port} --name={cn} {im} bash /prd/run.sh"
    cmd = cmd.format(port=port, gpu=gpu, cn=container_name, im=image_name,
                    directorypath=os.path.abspath(directory))
    print('Starting container on port {} using the command \n \n {}'.format(
        port, cmd))
    subprocess.check_call(cmd, shell=True)
    """
    NOTE: To make debugging cycles easier, there is a slight difference between
    how a model is orchestrated on the evaluation server and how it is
    orchestrated here in this test suite.
    On the server, repo2docker copies all the files to /home/crowdai inside
    the docker image, and we start the image by running `/home/crowdai/run.sh`
    but, in the test suite, we mount the working directory to `/prd` inside
    the container, and run it by executing `/prd/run.sh`.
    This removes the need to redundantly copy files over into the container
    when debugging your code with `--no-build` enabled.
    """

    # attach to container to print output in case of failures
    cmd = "docker attach {}".format(container_name)
    dump = subprocess.Popen(cmd, shell=True, stderr=subprocess.PIPE)

    # get IP
    form = '{{ .NetworkSettings.IPAddress }}'
    cmd = "docker inspect --format='{}' {}".format(form, container_name)
    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
    p.wait()
    ip = p.stdout.read()[:-1].decode('UTF-8')
    print('Received IP of server: ', ip)

    # start client model
    print('Connecting to model sever...', end="")
    url = 'http://{}:{}'.format(ip, port)
    model = adversarial_vision_challenge.TinyImageNetBSONModel(url)
    _wait_for_server_start(model)
    checkmark()

    print('Server version', model.server_version())
    # get predictions and/or gradient for model
    print('Checking model axis and bounds...', end="")
    channel_axis = model.channel_axis()

    assert channel_axis == 3, \
        "model channel axis should be 3, but is: %s" % channel_axis
    assert model.bounds() == (0, 255), \
        "model bounds should be (0,255), but got: %s" % model.bounds
    checkmark()

    # test predictions
    print('Checking return value...', end="")
    image, label = np.random.uniform(size=(64, 64, 3)).astype(
        np.float32) * 255, 1

    assert isinstance(model(image), int), \
        "prediction should be an int-value, but got: %s" % type(model(image))
    checkmark()

    # test prediction performance
    print('Testing model accuracy on test samples...')
    test_samples = adversarial_vision_challenge.utils.get_test_data()
    correct = 0

    for image, label in tqdm(test_samples):
        if model(image) == label:
            correct += 1

    print('The top-1 performance of your model is {}%.'.format(
        100 * correct / float(len(test_samples))))

    if correct / float(len(test_samples)) < 0.5:
        raise AssertionError(
            'The performance of your model is too low (< 50%)! Please check'
            ' whether your preprocessing is correctly implemented.')

    print('')
    print('All tests successful, have fun submitting!')
    print('')
    sys.exit()


if __name__ == '__main__':
    print('running {}'.format(os.path.basename(__file__)))
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "directory", help="The directory containing the Dockerfile.")
    parser.add_argument(
        "--no-cache", action='store_true',
        help="Disables the cache when building the model image.")
    parser.add_argument(
        "--no-build", action='store_true',
        help="Disables building the image, and assumes they exist. ")
    parser.add_argument(
        "--gpu", type=int, default=0, help="GPU number to run container on")
    args = parser.parse_args()
    test_model( args.directory, no_cache=args.no_cache,
                no_build=args.no_build, gpu=args.gpu)
